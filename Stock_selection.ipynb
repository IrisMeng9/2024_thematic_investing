{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/IrisMeng9/2024_thematic_investing/blob/main/Stock_selection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction\n",
        "This notebook details the complete process of stock selection in thematic investing, which is divided into three parts. We examine two themes: AI and Renewable Energy. The first two steps are the same for both themes, while the third step of selections is different.\n",
        "\n",
        "1. We tested 4 different search methods with Elasticsearch to find companies **generally related** to a specific theme. In this step, we displayed the first 200 search results for each method. The dataset is named as 'reference company desc'. You can skip the search process.   \n",
        "2. After having the search resutls from 4 different methods, we\n",
        "manually reviewed the results, labeling the companies as 1 if they are theme-related and 0 otherwise. This manual review was still relatively general, it allowed us to compare the accuracy of the four methods and select the best two: **ELSER and Exact Search**. We combined the data from these two sets of results to form a union set, then filtered out the data that was labeled as 1 in either set. These companies were used as the dataset for the next step.   \n",
        "3. As mentioned before, the methods used for sorting AI and Renwable Energy stocks were different.\n",
        "\n",
        "    3.1  For AI:    \n",
        "    In this step, we split the description of each company into individual sentences, use minilm model to transform the sentence and identify those related to specific theme. Then, we used BART zero-shot classification to label these sentences. We need to define our own labels, each associated with a score. After classfication, we add up the scores for each company and rank them in descending order to create our recommend list.   \n",
        "    \n",
        "    3.2  For Renewable Energy:\n",
        "    \n",
        "    In this step, we split the description of each company into sentences, and tag description based on theme keywords. Setting score rubric and scoring each of the company depends on their tags. Remove all the comapny with low irrelevance. After classfication, we add up the scores for each company and rank them in descending order to create our recommend list.  \n",
        "\n"
      ],
      "metadata": {
        "id": "M5rlH6VyQtBD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1: 4 search methods\n",
        "In this section, we present 4 search methods. Exact and BM25 are fast, while vector search and Elser will take longer. You can skip this cause we already have search results saved to a file.\n"
      ],
      "metadata": {
        "id": "EExZ0dGzQtEt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "WciLepZ8QjEp"
      },
      "outputs": [],
      "source": [
        "pip install elasticsearch"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exact match"
      ],
      "metadata": {
        "id": "HX7HUYSRVeN7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import elasticsearch as es\n",
        "from elasticsearch.helpers import BulkIndexError, bulk\n",
        "import time\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Elasticsearch\n",
        "es_idx_utestiam = 'utest-companydes'\n",
        "cnxn_es = es.Elasticsearch(\n",
        "    cloud_id='adt-search-es-dev:dXMtY2VudHJhbDEuZ2NwLmNsb3VkLmVzLmlvJDg1NjQzOWRkZjE1NjQzYTE5NzBlYjEwYTg1NDYyMTY4JGU2OWRmMmQ4MGJlOTRkMDc5ZGZkY2ZmZDNhMGUwYTBj',\n",
        "    api_key='S3ZuM21JOEJabFl1cG5MZEE0Tm46ejMwb2NtcW5TWi1xWW1OU1ZCRi0wQQ=='\n",
        ")\n",
        "\n",
        "start_time = time.time()  # timer start\n",
        "\n",
        "client_info = cnxn_es.info()\n",
        "print('Connected to Elasticsearch!')\n",
        "print(client_info.body)\n",
        "\n",
        "index_mapping = {\n",
        "    \"properties\": {\n",
        "        \"symbol\": {\"type\": \"keyword\"},\n",
        "        \"marketcap\": {\"type\": \"long\"},\n",
        "        \"cik\": {\"type\": \"integer\"},\n",
        "        \"description\": {\"type\": \"text\"},\n",
        "        \"sector\": {\"type\": \"text\"},\n",
        "        \"industry\": {\"type\": \"text\"},\n",
        "        \"country\": {\"type\": \"keyword\"},\n",
        "    }\n",
        "}\n",
        "\n",
        "# delete old and create new index\n",
        "cnxn_es.indices.delete(index=es_idx_utestiam, ignore_unavailable=True)\n",
        "cnxn_es.indices.create(index=es_idx_utestiam, mappings=index_mapping)\n",
        "print(f\"Created index {es_idx_utestiam}.\")\n",
        "\n",
        "# read/load to Elasticsearch\n",
        "file_path = '/content/drive/MyDrive/thematic/reference company desc.xlsx'\n",
        "dfg = pd.read_excel(file_path).replace({np.nan: None, pd.NA: None, \" \": None})\n",
        "dfg = dfg.fillna('')\n",
        "\n",
        "actions = []\n",
        "for index, row in dfg.iterrows():\n",
        "    d = {\n",
        "        \"_index\": es_idx_utestiam,\n",
        "        \"_op_type\": 'index',\n",
        "        \"_source\": {\n",
        "            \"symbol\": row[\"Symbol\"],\n",
        "            \"marketcap\": row[\"MktCap\"],\n",
        "            \"cik\": row[\"cik\"],\n",
        "            \"description\": row[\"description\"],\n",
        "            \"sector\": row[\"sector\"],\n",
        "            \"industry\": row[\"industry\"],\n",
        "            \"country\": row[\"country\"],\n",
        "        }\n",
        "    }\n",
        "    actions.append(d)\n",
        "try:\n",
        "    bulk(cnxn_es, actions)\n",
        "    print(\"Data loaded successfully.\")\n",
        "except BulkIndexError as e:\n",
        "    print(f\"Bulk indexing error: {e}\")\n",
        "    for error in e.errors:\n",
        "        print(error)\n",
        "except Exception as e:\n",
        "    print(f\"Error loading data: {e}\")\n",
        "\n",
        "cnxn_es.indices.refresh(index=es_idx_utestiam)\n",
        "result = cnxn_es.cat.count(index=es_idx_utestiam, format=\"json\")\n",
        "print(\"Document count:\", result)\n",
        "\n",
        "# def 200 companies\n",
        "def get_search_results(query, index, size=200, scroll='2m'):\n",
        "    data = []\n",
        "    result = cnxn_es.search(index=index, body=query, size=100, scroll=scroll)\n",
        "    scroll_id = result['_scroll_id']\n",
        "    hits = result['hits']['hits']\n",
        "    while len(hits) > 0 and len(data) < size:\n",
        "        for doc in hits:\n",
        "            if len(data) >= size:\n",
        "                break\n",
        "            record = {\n",
        "                \"Document ID\": doc['_id'],\n",
        "                \"Score\": doc['_score']\n",
        "            }\n",
        "            record.update(doc['_source'])\n",
        "            data.append(record)\n",
        "        result = cnxn_es.scroll(scroll_id=scroll_id, scroll=scroll)\n",
        "        scroll_id = result['_scroll_id']\n",
        "        hits = result['hits']['hits']\n",
        "    return data\n",
        "\n",
        "# Boolean query\n",
        "query_re = {\n",
        "    \"query\": {\n",
        "        \"bool\": {\n",
        "            \"should\": [\n",
        "                {\"match\": {\"description\": \"renewable\"}},\n",
        "                {\"match\": {\"description\": \"solar\"}},\n",
        "                {\"match\": {\"description\": \"wind\"}},\n",
        "                {\"match\": {\"description\": \"hydro\"}},\n",
        "                {\"match\": {\"description\": \"biomass\"}},\n",
        "                {\"match\": {\"description\": \"geothermal\"}},\n",
        "                {\"match\": {\"description\": \"tidal\"}},\n",
        "                {\"match\": {\"description\": \"photovoltaic\"}},\n",
        "                {\"match\": {\"description\": \"sustainable energy\"}},\n",
        "                {\"match\": {\"description\": \"green energy\"}}\n",
        "            ],\n",
        "            \"minimum_should_match\": 1\n",
        "        }\n",
        "    },\n",
        "    \"track_scores\": True\n",
        "}\n",
        "\n",
        "query_ai = {\n",
        "    \"query\": {\n",
        "        \"bool\": {\n",
        "            \"should\": [\n",
        "                {\"match\": {\"description\": \"machine learning\"}},\n",
        "                {\"match\": {\"description\": \"cloud\"}},\n",
        "                {\"match\": {\"description\": \"financial technology\"}},\n",
        "                {\"match\": {\"description\": \"natural language processing\"}},\n",
        "                {\"match\": {\"description\": \"computer vision\"}},\n",
        "                {\"match\": {\"description\": \"deep learning\"}},\n",
        "                {\"match\": {\"description\": \"autonomous vehicles\"}},\n",
        "                {\"match\": {\"description\": \"AI hardware\"}},\n",
        "                {\"match\": {\"description\": \"reinforcement learning\"}},\n",
        "                {\"match\": {\"description\": \"data mining\"}},\n",
        "                {\"match\": {\"description\": \"Chatbots\"}},\n",
        "                {\"match\": {\"description\": \"intelligent systems\"}},\n",
        "                {\"match\": {\"description\": \"robotics\"}},\n",
        "                {\"match\": {\"description\": \"AI algorithms\"}},\n",
        "                {\"match\": {\"description\": \"AI research\"}},\n",
        "                {\"match\": {\"description\": \"AI applications\"}}\n",
        "            ],\n",
        "            \"minimum_should_match\": 1\n",
        "        }\n",
        "    },\n",
        "    \"track_scores\": True\n",
        "}\n",
        "\n"
      ],
      "metadata": {
        "id": "upZ2mV4AVQE5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# AI\n",
        "start_ai_time = time.time()\n",
        "data_ai = get_search_results(query_ai, es_idx_utestiam)\n",
        "end_ai_time = time.time()\n",
        "\n",
        "df_ai = pd.DataFrame(data_ai)\n",
        "output_file_ai = 'AI_SearchResults.xlsx'\n",
        "df_ai.to_excel(output_file_ai, index=False, sheet_name='Exact Match')\n",
        "print(f\"Search results saved to {output_file_ai}\")\n",
        "print(f\"AI search time: {end_ai_time - start_ai_time:.2f} seconds\")"
      ],
      "metadata": {
        "id": "YkvvBr4ZubXX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Renewable Energy\n",
        "start_re_time = time.time()\n",
        "data_re = get_search_results(query_re, es_idx_utestiam)\n",
        "end_re_time = time.time()\n",
        "\n",
        "df_re = pd.DataFrame(data_re)\n",
        "output_file_re = 'Renewable_SearchResults.xlsx'\n",
        "df_re.to_excel(output_file_re, index=False, sheet_name='Exact Match')\n",
        "print(f\"Search results saved to {output_file_re}\")\n",
        "print(f\"Renewable Energy search time: {end_re_time - start_re_time:.2f} seconds\")"
      ],
      "metadata": {
        "id": "R4QLH0qestbE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### BM25"
      ],
      "metadata": {
        "id": "qYbWp6IZVS4W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##BM25\n",
        "def get_index_count(index_name):\n",
        "    return cnxn_es.cat.count(index=index_name, format=\"json\")\n",
        "\n",
        "def search_es(query_string, index_name):\n",
        "    query_body = {\n",
        "        \"size\":200,\n",
        "        \"query\": {\n",
        "            \"multi_match\": {\n",
        "                \"query\": query_string,\n",
        "                \"fields\": [\"description\"],  # Specify fields to search\n",
        "                \"fuzziness\": \"AUTO\",\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "    response = cnxn_es.search(index=index_name, body=query_body)\n",
        "    return response\n",
        "\n",
        "# Check index count\n",
        "index_count = get_index_count(es_idx_utestiam)\n",
        "print(\"Number of documents in index:\", index_count)\n",
        "\n",
        "# Perform search\n",
        "theme_AI = \"AI, Artificial Intelligence, machine learning, neural networks, deep learning, cloud computing, robotics\"\n",
        "\n",
        "search_result = search_es(theme_AI, es_idx_utestiam)\n",
        "print(\"Search Result:\", search_result)\n",
        "\n",
        "data = []\n",
        "for doc in search_result['hits']['hits']:\n",
        "    source = doc['_source']\n",
        "    source['document_id'] = doc['_id']  # Add document ID to the source data\n",
        "    source['score'] = doc['_score']  # Add score to the source data\n",
        "    data.append(source)\n",
        "\n",
        "# Create DataFrame\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "output_file = 'AI_SearchResults.xlsx'\n",
        "\n",
        "# append data to existing file\n",
        "with pd.ExcelWriter(output_file, mode='a', engine='openpyxl') as writer:\n",
        "    df.to_excel(writer, sheet_name='BM25', index=False)\n",
        "\n",
        "print(f\"New DataFrame has been successfully added to {output_file}\")"
      ],
      "metadata": {
        "id": "H57GmydAI9eS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "theme_renew = \"renewable energy, clean energy, solar energy, wind energy, hydroelectric power, biomass energy, geothermal energy, clean energy, sustainable energy\"\n",
        "\n",
        "search_result = search_es(theme_renew, es_idx_utestiam)\n",
        "print(\"Search Result:\", search_result)\n",
        "\n",
        "data = []\n",
        "for doc in search_result['hits']['hits']:\n",
        "    source = doc['_source']\n",
        "    source['document_id'] = doc['_id']  # Add document ID to the source data\n",
        "    source['score'] = doc['_score']  # Add score to the source data\n",
        "    data.append(source)\n",
        "\n",
        "# Create DataFrame\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "output_file = 'Renewable_SearchResults.xlsx'\n",
        "\n",
        "# append data to existing file\n",
        "with pd.ExcelWriter(output_file, mode='a', engine='openpyxl') as writer:\n",
        "    df.to_excel(writer, sheet_name='BM25', index=False)\n",
        "\n",
        "print(f\"New DataFrame has been successfully added to {output_file}\")"
      ],
      "metadata": {
        "id": "ruAFOaUbw6uZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Vector search"
      ],
      "metadata": {
        "id": "3QKnZscOVhvK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# pip install sentence-transformers\n",
        "!pip install sentence-transformers"
      ],
      "metadata": {
        "collapsed": true,
        "id": "KiIVFQjdx7lD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "# ELASTIC_CLOUD_ID = cloud_id\n",
        "# ELASTIC_API_KEY = api_key\n",
        "\n",
        "\n",
        "index_mapping = {\n",
        "    \"properties\": {\n",
        "        \"symbol\": {\"type\": \"keyword\"},\n",
        "        \"marketcap\": {\"type\": \"long\"},\n",
        "        \"cik\": {\"type\": \"integer\"},\n",
        "        \"description\": {\"type\": \"text\"},\n",
        "        \"sector\": {\"type\": \"text\"},\n",
        "        \"industry\": {\"type\": \"text\"},\n",
        "        \"country\": {\"type\": \"keyword\"},\n",
        "        \"embedding\": {'type': 'dense_vector',\n",
        "                      \"dims\": 384,\n",
        "                      \"index\": True,\n",
        "                      \"similarity\": \"l2_norm\",}, # for vector embedding\n",
        "        # \"elser_embedding\": {'type': 'sparse_vector'}, # for ELSER model\n",
        "    }\n",
        "}\n",
        "\n",
        "\n",
        "# create pipeline\n",
        "cnxn_es.ingest.put_pipeline(\n",
        "    id=\"vector-pipeline\",\n",
        "    description=\"Ingest pipeline for vector search\",\n",
        "    processors=[\n",
        "        {\n",
        "            \"inference\": {\n",
        "                \"model_id\": \"sentence-transformers__all-minilm-l6-v2\",\n",
        "                \"input_output\": [\n",
        "                    {\"input_field\": \"description\", \"output_field\": \"embedding\"}\n",
        "                ],\n",
        "            }\n",
        "        }\n",
        "    ],\n",
        ")\n",
        "\n",
        "print(\"Pipeline vector-pipeline has been created successfully.\")\n",
        "\n",
        "index_setting = {\n",
        "    \"index\": {\n",
        "        \"number_of_replicas\": \"1\",\n",
        "        \"number_of_shards\": \"1\",\n",
        "        \"default_pipeline\": \"vector-pipeline\",\n",
        "    }\n",
        "}\n",
        "\n",
        "\n",
        "SHOULD_DELETE_INDEX = True\n",
        "if SHOULD_DELETE_INDEX:\n",
        "    if cnxn_es.indices.exists(index=es_idx_utestiam):\n",
        "        print(\"Deleting existing %s\" % es_idx_utestiam)\n",
        "        cnxn_es.indices.delete(index=es_idx_utestiam, ignore_unavailable=True)\n",
        "\n",
        "print(\"Creating index %s\" % es_idx_utestiam)\n",
        "cnxn_es.indices.create(\n",
        "    index=es_idx_utestiam, mappings=index_mapping, settings=index_setting\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "knM6OT6sVjk4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dfg = pd.read_excel(\"reference company desc.xlsx\").replace({np.nan: None, pd.NA: None, \" \": None})\n",
        "dfg = dfg.fillna('')\n",
        "\n",
        "# # Replace NaN values with None to ensure compatibility with JSON, as JSON uses 'null' to represent missing values.\n",
        "actions = []\n",
        "for index, row in dfg.iterrows():\n",
        "    d = {\n",
        "        \"_index\": es_idx_utestiam,\n",
        "        \"_op_type\": 'index',\n",
        "        \"_source\": {\n",
        "            \"symbol\": row[\"Symbol\"],\n",
        "            \"marketcap\": row[\"MktCap\"],\n",
        "            \"cik\": row[\"cik\"],\n",
        "            \"description\": row[\"description\"],\n",
        "            \"sector\": row[\"sector\"],\n",
        "            \"industry\": row[\"industry\"],\n",
        "            \"country\": row[\"country\"],\n",
        "        }\n",
        "    }\n",
        "    actions.append(d)\n",
        "\n",
        "\n",
        "# for test, large data size will have time our error\n",
        "try:\n",
        "    eshelpers.bulk(cnxn_es, actions, pipeline=\"vector-pipeline\")\n",
        "    print(\"Data loaded successfully.\")\n",
        "except BulkIndexError as e:\n",
        "    print(f\"Bulk indexing error: {e}\")\n",
        "    for error in e.errors:\n",
        "        print(error)\n",
        "except Exception as e:\n",
        "    print(f\"Error loading data: {e}\")\n",
        "\n",
        "\n",
        "cnxn_es.indices.refresh(index=es_idx_utestiam)\n",
        "result = cnxn_es.cat.count(index=es_idx_utestiam, format=\"json\")\n",
        "print(\"Count Result:\", result)\n"
      ],
      "metadata": {
        "id": "n7xYhTyxnv4Y",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_cosine_similarity_query(query_vector):\n",
        "    return {\n",
        "        \"script_score\": {\n",
        "            \"query\": {\"match_all\": {}},\n",
        "            \"script\": {\n",
        "                \"source\": \"cosineSimilarity(params.query_vector, 'embedding') + 1.0\",\n",
        "                \"params\": {\"query_vector\": query_vector}\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "\n",
        "def esquery(query_vector, idx=None):\n",
        "    idx = idx if idx else es_idx_transcripts\n",
        "    query = create_cosine_similarity_query(query_vector)\n",
        "\n",
        "    r = cnxn_es.search(index=idx, body={\n",
        "        \"_source\": {\n",
        "            \"excludes\": []\n",
        "        },\n",
        "        \"query\": query,\n",
        "        \"size\": 200\n",
        "    })\n",
        "    print(f\"Total hits: {r['hits']['total']['value']}\")  # Adjusted for Elasticsearch 7.x\n",
        "\n",
        "    for hit in r['hits']['hits']:\n",
        "        source = hit['_source']\n",
        "        score = hit['_score']  # Accessing the score for each document\n",
        "        print(f\"Symbol: {source.get('symbol', 'N/A')}, Market Cap: {source.get('marketcap', 'N/A')}, \\\n",
        "    CIK: {source.get('cik', 'N/A')}, Description: {source.get('description', 'N/A')}, Score: {score}\")\n",
        "\n",
        "    # Normalize the results into a DataFrame\n",
        "    df = pd.json_normalize(r['hits']['hits'])\n",
        "\n",
        "    rows = []\n",
        "\n",
        "    # Iterate over each hit and collect required details\n",
        "    for hit in r['hits']['hits']:\n",
        "        source = hit['_source']\n",
        "        row = {\n",
        "            \"score\": hit['_score'],\n",
        "            \"symbol\": source.get('symbol', 'N/A'),\n",
        "            \"marketcap\": source.get('marketcap', 'N/A'),\n",
        "            \"cik\": source.get('cik', 'N/A'),\n",
        "            \"description\": source.get('description', 'N/A'),\n",
        "            \"sector\": source.get('sector', 'N/A'),\n",
        "            \"industry\": source.get('industry', 'N/A'),\n",
        "            \"country\": source.get('country', 'N/A')\n",
        "        }\n",
        "        rows.append(row)\n",
        "\n",
        "    df = pd.DataFrame(rows)\n",
        "    # df.to_excel('vector_search_results.xlsx', index=False)\n",
        "\n",
        "    return r, df"
      ],
      "metadata": {
        "id": "KGxkzREhn-9b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ai_query_text = \"AI, Artificial Intelligence, machine learning, neural networks, deep learning\"\n",
        "\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "query_vector = model.encode(ai_query_text)\n",
        "\n",
        "# Call the esquery function with the query vector\n",
        "response, df = esquery(query_vector, es_idx_utestiam)\n",
        "\n",
        "output_file = 'AI_SearchResults.xlsx'\n",
        "\n",
        "# append data to existing file\n",
        "with pd.ExcelWriter(output_file, mode='a', engine='openpyxl') as writer:\n",
        "    df.to_excel(writer, sheet_name='Vector', index=False)\n",
        "\n",
        "print(f\"New DataFrame has been successfully added to {output_file}\")"
      ],
      "metadata": {
        "id": "Bvlt5O-dn_qd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "renew_query_text = (\"renewable energy, solar energy, wind energy, hydroelectric power, biomass energy, geothermal energy, \"\n",
        "               \"clean energy, sustainable energy\")\n",
        "\n",
        "query_vector = model.encode(ai_query_text)\n",
        "\n",
        "# Call the esquery function with the query vector\n",
        "response, df = esquery(query_vector, es_idx_utestiam)\n",
        "\n",
        "output_file = 'Renewable_SearchResults.xlsx'\n",
        "\n",
        "# append data to existing file\n",
        "with pd.ExcelWriter(output_file, mode='a', engine='openpyxl') as writer:\n",
        "    df.to_excel(writer, sheet_name='Vector', index=False)\n",
        "\n",
        "print(f\"New DataFrame has been successfully added to {output_file}\")"
      ],
      "metadata": {
        "id": "6TczugOd9I5p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ELSER model (from elastic search)\n",
        "Notice: You will need about 30 mins to finish the search with ELSER model, the following codes are for demenstration, you can skip it"
      ],
      "metadata": {
        "id": "H9xGNThNVkDJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from elasticsearch import exceptions\n",
        "import time\n",
        "\n",
        "index_mapping = {\n",
        "    \"properties\": {\n",
        "        \"symbol\": {\"type\": \"keyword\"},\n",
        "        \"marketcap\": {\"type\": \"long\"},\n",
        "        \"cik\": {\"type\": \"integer\"},\n",
        "        \"description\": {\"type\": \"text\"},\n",
        "        \"sector\": {\"type\": \"text\"},\n",
        "        \"industry\": {\"type\": \"text\"},\n",
        "        \"country\": {\"type\": \"keyword\"},\n",
        "        'elser_embedding': {'type': 'sparse_vector'}, # for ELSER model\n",
        "    }\n",
        "}\n",
        "\n",
        "\n",
        "try:\n",
        "    cnxn_es.ml.delete_trained_model(model_id=\".elser_model_2\", force=True)\n",
        "    print(\"Model deleted successfully. Proceeding with model creation.\")\n",
        "except exceptions.NotFoundError:\n",
        "    print(\"Model does not exist. Proceeding with model creation.\")\n",
        "\n",
        "# create model\n",
        "cnxn_es.ml.put_trained_model(\n",
        "    model_id=\".elser_model_2\",\n",
        "    input={\"field_names\": [\"text_field\"]}\n",
        ")\n",
        "\n",
        "# define model\n",
        "while True:\n",
        "    status = cnxn_es.ml.get_trained_models(model_id=\".elser_model_2\", include=\"definition_status\")\n",
        "    if status[\"trained_model_configs\"][0][\"fully_defined\"]:\n",
        "        print(\"ELSER Model is downloaded and ready to be deployed.\")\n",
        "        break\n",
        "    else:\n",
        "        print(\"Waiting for ELSER Model to be fully defined.\")\n",
        "    time.sleep(5)\n",
        "\n",
        "# deploy\n",
        "cnxn_es.ml.start_trained_model_deployment(\n",
        "    model_id=\".elser_model_2\",\n",
        "    number_of_allocations=1,\n",
        "    wait_for=\"starting\"\n",
        ")\n",
        "\n",
        "# deployment status\n",
        "while True:\n",
        "    status = cnxn_es.ml.get_trained_models_stats(model_id=\".elser_model_2\")\n",
        "    if status[\"trained_model_stats\"][0][\"deployment_stats\"][\"state\"] == \"started\":\n",
        "        print(\"ELSER Model has been successfully deployed.\")\n",
        "        break\n",
        "    else:\n",
        "        print(\"ELSER Model deployment is in progress.\")\n",
        "    time.sleep(5)\n",
        "\n",
        "# create pipeline\n",
        "cnxn_es.ingest.put_pipeline(\n",
        "    id=\"elser-ingest-pipeline\",\n",
        "    description=\"Ingest pipeline for ELSER\",\n",
        "    processors=[\n",
        "        {\n",
        "            \"inference\": {\n",
        "                \"model_id\": \".elser_model_2\",\n",
        "                \"input_output\": [\n",
        "                    {\"input_field\": \"description\", \"output_field\": \"elser_embedding\"}\n",
        "                ],\n",
        "            }\n",
        "        }\n",
        "    ],\n",
        ")\n",
        "\n",
        "print(\"Pipeline 'elser-ingest-pipeline' has been created successfully.\")\n",
        "\n",
        "# delete the index first in case it already exists\n",
        "cnxn_es.indices.delete(index=es_idx_utestiam, ignore_unavailable=True)\n",
        "cnxn_es.indices.create(index=es_idx_utestiam,\n",
        "                           settings={\"index\": {\"default_pipeline\": \"elser-ingest-pipeline\"}},\n",
        "                           mappings=index_mapping,\n",
        "                           )"
      ],
      "metadata": {
        "id": "0PjWP-_sVpNe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import elasticsearch.helpers as eshelpers\n",
        "from elasticsearch.helpers import BulkIndexError\n",
        "\n",
        "\n",
        "dfg = pd.read_excel(\"reference company desc.xlsx\").replace({np.nan: None, pd.NA: None, \" \":None})\n",
        "# Specifically fill missing values in the 'description' column with an empty string\n",
        "dfg = dfg.fillna('')\n",
        "\n",
        "# dfg = dfg[:500]\n",
        "\n",
        "\n",
        "actions = []\n",
        "for index, row in dfg.iterrows():\n",
        "    d = {\n",
        "        \"_index\": es_idx_utestiam,\n",
        "        \"_op_type\": 'index',\n",
        "        \"_source\": {\n",
        "            \"symbol\": row[\"Symbol\"],\n",
        "            \"marketcap\": row[\"MktCap\"],\n",
        "            \"cik\": row[\"cik\"],\n",
        "            \"description\": row[\"description\"],\n",
        "            \"sector\": row[\"sector\"],\n",
        "            \"industry\": row[\"industry\"],\n",
        "            \"country\": row[\"country\"],\n",
        "        }\n",
        "    }\n",
        "    actions.append(d)\n",
        "\n",
        "\n",
        "batch_size = 100\n",
        "\n",
        "def load_batch(es_connection, actions):\n",
        "    try:\n",
        "        eshelpers.bulk(es_connection, actions)\n",
        "        print(\"Batch loaded successfully.\")\n",
        "    except BulkIndexError as e:\n",
        "        print(f\"Bulk indexing error: {e}\")\n",
        "        for error in e.errors:\n",
        "            print(error)\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading data: {e}\")\n",
        "\n",
        "# smaller size\n",
        "for i in range(0, len(actions), batch_size):\n",
        "    batch = actions[i:i + batch_size]\n",
        "    load_batch(cnxn_es, batch)\n",
        "\n",
        "cnxn_es.indices.refresh(index=es_idx_utestiam)\n",
        "result = cnxn_es.cat.count(index=es_idx_utestiam, format=\"json\")\n",
        "print(\"Count Result:\", result)"
      ],
      "metadata": {
        "id": "yGuc6ZJIj8uN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def esquery1(query, idx=None):\n",
        "    idx = idx if idx else es_idx_transcripts\n",
        "\n",
        "    r = cnxn_es.search(index=idx, body={\n",
        "        \"_source\": {\n",
        "            \"excludes\": []\n",
        "        },\n",
        "        \"query\": query,\n",
        "        \"size\": 200\n",
        "    })\n",
        "    print(f\"Total hits: {r['hits']['total']['value']}\")  # Adjusted for Elasticsearch 7.x\n",
        "\n",
        "    for hit in r['hits']['hits']:\n",
        "        source = hit['_source']\n",
        "        score = hit['_score']  # Accessing the score for each document\n",
        "\n",
        "        # Printing all required details including the score\n",
        "        print(f\"Symbol: {source.get('symbol', 'N/A')}, Market Cap: {source.get('marketcap', 'N/A')}, \\\n",
        "    CIK: {source.get('cik', 'N/A')}, Description: {source.get('description', 'N/A')}, Score: {score}\")\n",
        "\n",
        "    # Normalize the results into a DataFrame\n",
        "    df = pd.json_normalize(r['hits']['hits'])\n",
        "\n",
        "    rows = []\n",
        "\n",
        "    # Iterate over each hit and collect required details\n",
        "    for hit in r['hits']['hits']:\n",
        "        source = hit['_source']\n",
        "        row = {\n",
        "            \"score\": hit['_score'],  # Include the score\n",
        "            \"symbol\": source.get('symbol', 'N/A'),\n",
        "            \"marketcap\": source.get('marketcap', 'N/A'),\n",
        "            \"cik\": source.get('cik', 'N/A'),\n",
        "            \"description\": source.get('description', 'N/A'),\n",
        "            \"sector\": source.get('sector', 'N/A'),\n",
        "            \"industry\": source.get('industry', 'N/A'),\n",
        "            \"country\": source.get('country', 'N/A')\n",
        "        }\n",
        "        rows.append(row)\n",
        "\n",
        "    # Create a DataFrame from the collected rows\n",
        "    df = pd.DataFrame(rows)\n",
        "\n",
        "    # Save the DataFrame to an Excel file\n",
        "    # df.to_excel('es_search_results.xlsx', index=False)\n",
        "\n",
        "    return r, df"
      ],
      "metadata": {
        "id": "AwB0N3xhkgDK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "theme_AI = (\"AI, cloud computing, robotics, NLP, Artificial Intelligence, machine learning, neural networks, \"\n",
        "            \"deep learning\")\n",
        "\n",
        "# ELSER Model search\n",
        "query = {\n",
        "                \"text_expansion\": {\n",
        "                    \"elser_embedding\": {\n",
        "                        \"model_id\": \".elser_model_2\",\n",
        "                        \"model_text\": theme_AI,\n",
        "                    }\n",
        "               }\n",
        "}\n",
        "r, dfr = esquery1(query, es_idx_utestiam)\n",
        "\n",
        "output_file = 'AI_SearchResults.xlsx'\n",
        "\n",
        "# append data to existing file\n",
        "with pd.ExcelWriter(output_file, mode='a', engine='openpyxl') as writer:\n",
        "    dfr.to_excel(writer, sheet_name='ELSER', index=False)\n",
        "\n",
        "print(f\"New DataFrame has been successfully added to {output_file}\")"
      ],
      "metadata": {
        "id": "LFdtz_bckDXm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "theme_Renew = (\"renewable energy, clean energy, solar energy, wind energy, hydroelectric power, biomass energy, \"\n",
        "               \"geothermal energy, clean energy, sustainable energy, hydroelectric, electric vehicle, biofuel\")\n",
        "\n",
        "# ELSER Model search\n",
        "query = {\n",
        "                \"text_expansion\": {\n",
        "                    \"elser_embedding\": {\n",
        "                        \"model_id\": \".elser_model_2\",\n",
        "                        \"model_text\": theme_Renew,\n",
        "                    }\n",
        "               }\n",
        "}\n",
        "r, dfr = esquery1(query, es_idx_utestiam)\n",
        "\n",
        "output_file = 'Renewable_SearchResults.xlsx'\n",
        "\n",
        "# append data to existing file\n",
        "with pd.ExcelWriter(output_file, mode='a', engine='openpyxl') as writer:\n",
        "    dfr.to_excel(writer, sheet_name='ELSER', index=False)\n",
        "\n",
        "print(f\"New DataFrame has been successfully added to {output_file}\")"
      ],
      "metadata": {
        "id": "10QPlUmw2E5x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2: AI Manual check\n",
        "We manually reviewed the search results and added a column called 'Flag' to show. We will use the labeled datasets, both of which are located in the folder. The names of the datasets are AI_SearchResults and Renewable_SearchResults.\n",
        "\n",
        "Please **change** the file path that contains AI_SearchResults.xlsx"
      ],
      "metadata": {
        "id": "0dZ0aTjsVqtW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "Z_jMn6b5uiy7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Remeber to change the file path ###\n",
        "file_path = '/content/drive/MyDrive/thematic/AI/AI_SearchResults.xlsx'\n",
        "\n",
        "df_els = pd.read_excel(file_path, sheet_name='ELSER')\n",
        "df_exact = pd.read_excel(file_path, sheet_name='Exact Match')\n",
        "columns_to_keep = ['symbol', 'marketcap', 'cik', 'description', 'sector', 'industry', 'country', 'Flag']\n",
        "\n",
        "# company whose flag = 1\n",
        "df_els_filtered = df_els[df_els['Flag'] == 1][columns_to_keep]\n",
        "df_exact_filtered = df_exact[df_exact['Flag'] == 1][columns_to_keep]\n",
        "\n",
        "combined_df = pd.concat([df_els_filtered, df_exact_filtered])\n",
        "\n",
        "unique_combined_df = combined_df.drop_duplicates(subset='symbol')\n",
        "unique_combined_df.reset_index(drop=True, inplace=True)\n",
        "print(unique_combined_df.head())\n"
      ],
      "metadata": {
        "id": "WriCRW-BVumG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unique_combined_df.to_excel('unique_combined_df.xlsx', index=False)"
      ],
      "metadata": {
        "id": "PX7d0LPqv2uI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3 for AI: Bart classification\n"
      ],
      "metadata": {
        "id": "yZvZKUdwVvb3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "GKOE4DzBI-Dk"
      },
      "outputs": [],
      "source": [
        "!pip install transformers\n",
        "!pip install torch\n",
        "!pip install sentence-transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import numpy as np\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "\n",
        "from transformers import pipeline\n",
        "# Choose the model and task\n",
        "model_name = 'facebook/bart-large-mnli'\n",
        "classifier = pipeline('zero-shot-classification', model=model_name)"
      ],
      "metadata": {
        "id": "uANee1Q0O-Ns"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load the model\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')"
      ],
      "metadata": {
        "id": "f_eVe6vJVp4a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "candidate_labels = [\"GPU\", \"Semiconductor vendors\", \"Data center\", \"AI software provider\", \"Computing\" ,\"AI service\", \"AI consultancy\"]\n",
        "ai_query = \"This sentence is about GPU, artificial intelligence, machine learning, computing, neural networks, and deep learning.\"\n",
        "\n",
        "def split_into_sentences(text):\n",
        "    # Regular expression to split text into sentences\n",
        "    sentence_endings = re.compile(r'(?<!\\bInc)(?<!\\bLtd)(?<!\\bCo)(?<!\\bCorp)(?<!\\bInc\\.)(?<!\\bLtd\\.)(?<!\\bCo\\.)(?<!\\bCorp\\.)(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?)\\s')\n",
        "    sentences = sentence_endings.split(text)\n",
        "    return sentences\n",
        "\n",
        "def extract_ai_sentences(sentences):\n",
        "    # Encode the sentences\n",
        "    sentence_embeddings = model.encode(sentences, convert_to_tensor=True)\n",
        "    query_embedding = model.encode(ai_query, convert_to_tensor=True)\n",
        "    # Compute cosine similarities\n",
        "    cos_scores = util.pytorch_cos_sim(query_embedding, sentence_embeddings)[0]\n",
        "    # Filter sentences based on cosine similarity\n",
        "    threshold = 0.3\n",
        "    ai_sentences = [sentences[i] for i in range(len(sentences)) if cos_scores[i] > threshold]\n",
        "    return ai_sentences"
      ],
      "metadata": {
        "id": "Z0xDKuelRDlk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def classify_sentences(sentences):\n",
        "    # Classify sentences using the BART model\n",
        "    results = []\n",
        "    for sentence in sentences:\n",
        "        result = classifier(sentence, candidate_labels=candidate_labels,multi_label=True)\n",
        "        label = result['labels'][0]\n",
        "        score = result['scores'][0]\n",
        "        results.append((label, score))\n",
        "    return results"
      ],
      "metadata": {
        "id": "6VSF3WPFURTY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_rows = []\n",
        "for index, row in unique_combined_df.iterrows():\n",
        "\n",
        "    sentences = split_into_sentences(row['description'])\n",
        "    ai_related_sentences = extract_ai_sentences(sentences)\n",
        "\n",
        "\n",
        "    sentence_classifications = classify_sentences(ai_related_sentences)\n",
        "\n",
        "    for sentence, classification in zip(ai_related_sentences, sentence_classifications):\n",
        "        new_row = {\n",
        "            'symbol': row['symbol'],\n",
        "            'marketcap': row['marketcap'],\n",
        "            'cik': row['cik'],\n",
        "            'description': row['description'],\n",
        "            'sector': row['sector'],\n",
        "            'industry': row['industry'],\n",
        "            'country': row['country'],\n",
        "            'sentences': sentence,\n",
        "            'AI_label': classification[0],  #\n",
        "            'score': classification[1]       #\n",
        "        }\n",
        "        new_rows.append(new_row)"
      ],
      "metadata": {
        "id": "jwiGdZSGRDs5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_data = pd.DataFrame(new_rows)\n",
        "classified_data = new_data.sort_values(by='symbol').reset_index()"
      ],
      "metadata": {
        "id": "ELfGaPaARDxE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Assign score and rank"
      ],
      "metadata": {
        "id": "yfLA186Ipksu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "label_scores = {\n",
        "    \"GPU\": 5,\n",
        "    \"Semiconductor vendors\": 4,\n",
        "    \"Data center\": 4,\n",
        "    \"AI software provider\": 3,\n",
        "    \"Computing\": 3.5,\n",
        "    \"AI service\": 2,\n",
        "    \"AI consultancy\": 1\n",
        "}\n",
        "\n",
        "def calculate_exposure_score(label, score):\n",
        "    if score < 0.75:\n",
        "        return 0\n",
        "    else:\n",
        "        return label_scores.get(label, 0)"
      ],
      "metadata": {
        "id": "jV7skiPXSwL6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classified_data['exposure_score'] = classified_data.apply(lambda row: calculate_exposure_score(row['AI_label'], row['score']), axis=1)\n",
        "\n",
        "# Sum up exposure scores for each company\n",
        "exposure_sum = classified_data.groupby('symbol')['exposure_score'].sum().reset_index()\n",
        "\n",
        "# Merge exposure_sum back into classified_data on 'symbol'\n",
        "classified_data = classified_data.merge(exposure_sum, on='symbol', how='left', suffixes=(None, '_sum'))\n",
        "\n",
        "# Rename the new column to exposure_sum\n",
        "classified_data.rename(columns={'exposure_score_sum': 'exposure_sum'}, inplace=True)"
      ],
      "metadata": {
        "id": "DbBmr57oY6da"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classified_data.head()"
      ],
      "metadata": {
        "id": "vEHf0T2uY8dn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merged_data = classified_data.groupby(['symbol', 'marketcap', 'cik', 'description', 'sector', 'industry', 'country', 'exposure_sum']).agg({\n",
        "    'AI_label': lambda x: ', '.join(set(x)),}).reset_index()\n",
        "f_list = merged_data.copy()\n",
        "\n",
        "# Sort by exposure_sum in descending order\n",
        "f_list.sort_values(by='exposure_sum', ascending=False, inplace=True)\n",
        "f_list.to_excel('/content/drive/MyDrive/thematic/AI/AI_rec_list.xlsx', index=False)"
      ],
      "metadata": {
        "id": "o6fZr0B_FaRt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2: Renewable Manual check\n",
        "Please **change** the file path that contains Renewable_SearchResults.xlsx"
      ],
      "metadata": {
        "id": "uqv_lggaeONS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "### Remeber to change the file path ###\n",
        "file_path = '/content/drive/MyDrive/thematic/Renewable/Renewable_SearchResults.xlsx'\n",
        "df_els = pd.read_excel(file_path, sheet_name='ELSER')\n",
        "df_exact = pd.read_excel(file_path, sheet_name='Exact Match')\n",
        "columns_to_keep = ['symbol', 'marketcap', 'cik', 'description', 'sector', 'industry', 'country', 'Flag']\n",
        "\n",
        "# company whose flag = 1\n",
        "df_els_filtered = df_els[df_els['Flag'] == 1][columns_to_keep]\n",
        "df_exact_filtered = df_exact[df_exact['Flag'] == 1][columns_to_keep]\n",
        "\n",
        "combined_df = pd.concat([df_els_filtered, df_exact_filtered])\n",
        "\n",
        "data = combined_df.drop_duplicates(subset='symbol')\n",
        "data.reset_index(drop=True, inplace=True)\n",
        "print(data.head())\n"
      ],
      "metadata": {
        "id": "dSPhPYeFeGgv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3 for Renewable Energy: Keyword Search\n"
      ],
      "metadata": {
        "id": "PHarPBSK2T9p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define theme keywords\n",
        "theme_keywords = [\n",
        "    \"solar\", \"wind\", \"hydrogen fueling\", \"hydro\", \"biomass\", \"biofuels\",\n",
        "    \"geothermal\", \"ethanol\", \"natural gas\", \"oil\", \"nuclear\", \"coal\", \"methane\"\n",
        "]\n",
        "\n",
        "# Function to tag description based on theme keywords\n",
        "def tag_description(description, keywords):\n",
        "    sentences = re.split(r'(?<=[.!?]) +', description)\n",
        "    tags = []\n",
        "    for sentence in sentences:\n",
        "        for keyword in keywords:\n",
        "            if re.search(keyword, sentence, re.IGNORECASE):\n",
        "                tags.append(keyword)\n",
        "    return list(set(tags))  # Return unique tags\n",
        "\n",
        "# Apply the tag_description function to each description\n",
        "data['description_tag'] = data['description'].apply(lambda x: tag_description(x, theme_keywords))\n",
        "\n",
        "# Remove rows with empty description_tag and drop 'normalized score' column\n",
        "data = data[data['description_tag'].map(len) > 0]\n",
        "# data = data.drop(columns=['Normalized Score'])\n",
        "\n",
        "# Define scoring rules\n",
        "def calculate_score(tags):\n",
        "    score = 0\n",
        "\n",
        "    if len(tags) == 1 and tags[0] in [\"solar\", \"wind\", \"hydrogen fueling\", \"hydro\", \"biomass\", \"biofuels\", \"geothermal\", \"ethanol\"]:\n",
        "        score += 50\n",
        "    if \"natural gas\" in tags:\n",
        "        score -= 15\n",
        "    if 1 <= len(tags) <= 3 and \"solar\" in tags:\n",
        "        score += 15\n",
        "    if 4 <= len(tags) <= 6 and \"solar\" in tags and not any(kw in tags for kw in [\"natural gas\", \"oil\", \"nuclear\", \"coal\", \"methane\"]):\n",
        "        score += 8\n",
        "    if 2 <= len(tags) <= 3 and \"wind\" in tags and not any(kw in tags for kw in [\"natural gas\", \"oil\"]):\n",
        "        score += 15\n",
        "    if len(tags) == 1 and tags[0] in [\"natural gas\", \"oil\"]:\n",
        "        score -= 50\n",
        "    if \"nuclear\" in tags:\n",
        "        score -= 15\n",
        "    if \"coal\" in tags:\n",
        "        score -= 15\n",
        "    if 1 <= len(tags) <= 3 and \"hydro\" in tags:\n",
        "        score += 15\n",
        "    if \"oil\" in tags:\n",
        "        score -= 15\n",
        "    if \"ethanol\" in tags:\n",
        "        score -= 15\n",
        "    if \"biofuels\" in tags:\n",
        "        score += 15\n",
        "    if \"methane\" in tags:\n",
        "        score -= 15\n",
        "\n",
        "    return score\n",
        "\n",
        "# Apply scoring rules to each company\n",
        "data['score'] = data['description_tag'].apply(lambda x: calculate_score(x))\n",
        "\n",
        "# Filter out companies in the \"Building Products & Equipment\" industry\n",
        "data = data[data['industry'] != \"Building Products & Equipment\"]\n",
        "\n",
        "# Separate the calculation of keyword counts\n",
        "data['keyword_count'] = data['description_tag'].apply(lambda x: len(x))\n",
        "\n",
        "# Sort by score and keyword count\n",
        "data = data.sort_values(by=['score', 'keyword_count'], ascending=[False, True])\n",
        "\n",
        "# Drop the temporary 'keyword_count' column\n",
        "data = data.drop(columns=['keyword_count'])\n",
        "\n",
        "# Step 1: Remove companies with score less than 0\n",
        "data = data[data['score'] >= 0]\n",
        "\n",
        "# Step 2: Adjust ranking for companies with score 0\n",
        "zero_score_data = data[data['score'] == 0]\n",
        "non_zero_score_data = data[data['score'] != 0]\n",
        "\n",
        "# Further sorting for zero score companies\n",
        "utilities_regulated_electric = zero_score_data[zero_score_data['industry'] == \"UtilitiesRegulated Electric\"]\n",
        "other_industries = zero_score_data[zero_score_data['industry'] != \"UtilitiesRegulated Electric\"]\n",
        "\n",
        "# Concatenate the dataframes\n",
        "sorted_zero_score_data = pd.concat([other_industries, utilities_regulated_electric])\n",
        "data = pd.concat([non_zero_score_data, sorted_zero_score_data])\n",
        "\n",
        "# Adjust ranking for companies with score 65\n",
        "score_65_data = data[data['score'] == 65]\n",
        "non_score_65_data = data[data['score'] != 65]\n",
        "\n",
        "# Further sorting for score 65 companies\n",
        "solar_industry = score_65_data[score_65_data['industry'].str.contains(\"solar\", case=False, na=False)]\n",
        "other_industries = score_65_data[~score_65_data['industry'].str.contains(\"solar\", case=False, na=False)]\n",
        "\n",
        "# Concatenate the dataframes\n",
        "sorted_score_65_data = pd.concat([solar_industry, other_industries])\n",
        "data = pd.concat([sorted_score_65_data, non_score_65_data])\n",
        "\n",
        "# Adjust ranking for companies with score 0 and sector \"Utilities\"\n",
        "zero_score_data = data[data['score'] == 0]\n",
        "non_zero_score_data = data[data['score'] != 0]\n",
        "\n",
        "# Further sorting for zero score companies\n",
        "utilities_sector = zero_score_data[zero_score_data['sector'] == \"Utilities\"]\n",
        "other_sectors = zero_score_data[zero_score_data['sector'] != \"Utilities\"]\n",
        "\n",
        "# Concatenate the dataframes\n",
        "sorted_zero_score_data = pd.concat([other_sectors, utilities_sector])\n",
        "data = pd.concat([non_zero_score_data, sorted_zero_score_data])\n",
        "\n",
        "# Save the resulting dataframe to an Excel file\n",
        "final_sorted_excel_file_path = '/content/drive/MyDrive/thematic/Renewable/Renew_rec_list.xlsx'\n",
        "data.to_excel(final_sorted_excel_file_path, index=False)\n",
        "\n",
        "print(f\"The final sorted data has been saved to: {final_sorted_excel_file_path}\")"
      ],
      "metadata": {
        "id": "AMxHQw_J2YdZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}